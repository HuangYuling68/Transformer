{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16f229ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "698383c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    缩放点积注意力计算。\n",
    "    \n",
    "    参数:\n",
    "        Q: 查询矩阵 (batch_size, seq_len_q, embed_size)\n",
    "        K: 键矩阵 (batch_size, seq_len_k, embed_size)\n",
    "        V: 值矩阵 (batch_size, seq_len_v, embed_size)\n",
    "        mask: 掩码矩阵，用于屏蔽不应该关注的位置 (可选)\n",
    "\n",
    "    返回:\n",
    "        output: 注意力加权后的输出矩阵\n",
    "        attention_weights: 注意力权重矩阵\n",
    "    \"\"\"\n",
    "    embed_size = Q.size(-1)\n",
    "    scores = torch.matmul(Q, K.transpose(-2,-1)) / math.sqrt(embed_size)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93a0f469",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        \"\"\"\n",
    "        单头注意力机制。\n",
    "        \n",
    "        参数:\n",
    "            embed_size: 输入序列（Inputs）的嵌入（Input Embedding）维度，也是论文中所提到的d_model。\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.w_q = nn.Linear(embed_size, embed_size)\n",
    "        self.w_k = nn.Linear(embed_size, embed_size)\n",
    "        self.w_v = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "        \n",
    "        参数:\n",
    "            q: 查询矩阵 (batch_size, seq_len_q, embed_size)\n",
    "            k: 键矩阵 (batch_size, seq_len_k, embed_size)\n",
    "            v: 值矩阵 (batch_size, seq_len_v, embed_size)\n",
    "            mask: 掩码矩阵，用于屏蔽不应关注的位置 (batch_size, seq_len_q, seq_len_k)\n",
    "\n",
    "        返回:\n",
    "            out: 注意力加权后的输出\n",
    "            attention_weights: 注意力权重矩阵\n",
    "        \"\"\"\n",
    "        Q = self.w_q(q)\n",
    "        K = self.w_k(k)\n",
    "        V = self.w_v(v)\n",
    "        out, attention_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e12e73a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        \"\"\"\n",
    "        自注意力（Self-Attention）机制。\n",
    "        \n",
    "        参数:\n",
    "            embed_size: 输入序列的嵌入维度（每个向量的特征维度）。\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.attention = Attention(embed_size)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "        \n",
    "        参数:\n",
    "            x: 输入序列 (batch_size, seq_len, embed_size)\n",
    "            mask: 掩码矩阵 (batch_size, seq_len, seq_len)\n",
    "\n",
    "        返回:\n",
    "            out: 自注意力加权后的输出 (batch_size, seq_len, embed_size)\n",
    "            attention_weights: 注意力权重矩阵 (batch_size, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        # 在自注意力机制中，q, k, v 都来自同一输入序列\n",
    "        # q = k = v = x\n",
    "        out, attention_weights = self.attention(x, x, x, mask)\n",
    "\n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6b232c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        \"\"\"\n",
    "        交叉注意力（Cross-Attention）机制。\n",
    "        \n",
    "        参数:\n",
    "            embed_size: 输入序列的嵌入维度。\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.attention = Attention(embed_size)\n",
    "    \n",
    "    def forward(self, q, kv, mask=None):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "        \n",
    "        参数:\n",
    "            query: 查询矩阵的输入 (batch_size, seq_len_q, embed_size)\n",
    "            kv: 键和值矩阵的输入 (batch_size, seq_len_kv, embed_size)\n",
    "            mask: 掩码矩阵 (batch_size, seq_len_q, seq_len_kv)\n",
    "\n",
    "        返回:\n",
    "            out: 注意力加权后的输出 (batch_size, seq_len_q, embed_size)\n",
    "            attention_weights: 注意力权重矩阵 (batch_size, seq_len_kv, seq_len_kv)\n",
    "        \"\"\"\n",
    "        # 在交叉注意力机制中，q 和 k, v 不同\n",
    "        # q 来自解码器，k 和 v 来自编码器（观察模型架构图）\n",
    "        out, attention_weights = self.attention(q, kv, kv, mask)\n",
    "\n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb9f213e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    缩放点积注意力计算。\n",
    "    参数:\n",
    "        Q: 查询矩阵 (batch_size, num_heads, seq_len_q, head_dim)\n",
    "        K: 键矩阵 (batch_size, num_heads, seq_len_k, head_dim)\n",
    "        V: 值矩阵 (batch_size, num_heads, seq_len_v, head_dim)\n",
    "        mask: 掩码矩阵 (1, 1, seq_len_q, seq_len_k) 或 (batch_size, 1, seq_len_q, seq_len_k) 或 (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "    返回:\n",
    "        output: 注意力加权后的输出矩阵\n",
    "        attention_weights: 注意力权重矩阵\n",
    "    \"\"\"\n",
    "    embed_size = Q.size(-1)\n",
    "    scores = nn.matmul(Q, K.transpose(-2,-1))/math.sqrt(embed_size)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(0,float('-inf'))\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    output = nn.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afad6bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, h):\n",
    "        \"\"\"\n",
    "        多头注意力机制：每个头单独定义线性层。\n",
    "        \n",
    "        参数:\n",
    "            d_model: 输入序列的嵌入维度。\n",
    "            h: 注意力头的数量。\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert d_model % h == 0, \"d_model 必须能被 h 整除。\"\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "        \n",
    "        参数:\n",
    "            q: 查询矩阵 (batch_size, seq_len_q, d_model)\n",
    "            k: 键矩阵 (batch_size, seq_len_k, d_model)\n",
    "            v: 值矩阵 (batch_size, seq_len_v, d_model)\n",
    "            mask: 掩码矩阵 (batch_size, 1, seq_len_q, seq_len_k)\n",
    "\n",
    "        返回:\n",
    "            out: 注意力加权后的输出\n",
    "            attention_weights: 注意力权重矩阵\n",
    "        \"\"\"\n",
    "        batch_size = q.size(0)\n",
    "        seq_len_q = q.size(1)\n",
    "        seq_len_k = v.size(1)\n",
    "        Q = self.w_q(q).view(batch_size, seq_len_q, self.h, -1).transpose(1,2)\n",
    "        K = self.w_k(k).view(batch_size, seq_len_k, self.h, -1).transpose(1,2)\n",
    "        V = self.w_v(v).view(batch_size, seq_len_k, self.h, -1).transpose(1,2)\n",
    "        scaled_attention, _ = scaled_dot_product_attention(Q, K, V, mask)\n",
    "        concat_out = scaled_attention.transpose(1,2).contigous().view(batch_size, -1, self.d_model)\n",
    "        out = self.fc_out(concat_out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0474f278",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositonwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        位置前馈网络。\n",
    "        \n",
    "        参数:\n",
    "            d_model: 输入和输出向量的维度\n",
    "            d_ff: FFN 隐藏层的维度，或者说中间层\n",
    "            dropout: 随机失活率（Dropout），即随机屏蔽部分神经元的输出，用于防止过拟合\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 先经过第一个线性层和 ReLU，然后经过第二个线性层\n",
    "        return self.w_2(self.dropout(self.w_1(x).relu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17a4b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        \"\"\"\n",
    "        残差连接，用于在每个子层后添加残差连接和 Dropout。\n",
    "        \n",
    "        参数:\n",
    "            dropout: Dropout 概率，用于在残差连接前应用于子层输出，防止过拟合。\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "        \n",
    "        参数:\n",
    "            x: 残差连接的输入张量，形状为 (batch_size, seq_len, d_model)。\n",
    "            sublayer: 子层模块的函数，多头注意力或前馈网络。\n",
    "\n",
    "        返回:\n",
    "            经过残差连接和 Dropout 处理后的张量，形状为 (batch_size, seq_len, d_model)。\n",
    "        \"\"\"\n",
    "        return x+self.dropout(sublayer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3306f9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, feature_size, epsilon=1e-9):\n",
    "        \"\"\"\n",
    "        层归一化，用于对最后一个维度进行归一化。\n",
    "        \n",
    "        参数:\n",
    "            feature_size: 输入特征的维度大小，即归一化的特征维度。\n",
    "            epsilon: 防止除零的小常数。\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.gamma=\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
