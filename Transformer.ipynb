{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16f229ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "698383c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    缩放点积注意力计算。\n",
    "    \n",
    "    参数:\n",
    "        Q: 查询矩阵 (batch_size, seq_len_q, embed_size)\n",
    "        K: 键矩阵 (batch_size, seq_len_k, embed_size)\n",
    "        V: 值矩阵 (batch_size, seq_len_v, embed_size)\n",
    "        mask: 掩码矩阵，用于屏蔽不应该关注的位置 (可选)\n",
    "\n",
    "    返回:\n",
    "        output: 注意力加权后的输出矩阵\n",
    "        attention_weights: 注意力权重矩阵\n",
    "    \"\"\"\n",
    "    embed_size = Q.size(-1)\n",
    "    scores = torch.matmul(Q, K.transpose(-2,-1)) / math.sqrt(embed_size)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93a0f469",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        \"\"\"\n",
    "        单头注意力机制。\n",
    "        \n",
    "        参数:\n",
    "            embed_size: 输入序列（Inputs）的嵌入（Input Embedding）维度，也是论文中所提到的d_model。\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.w_q = nn.Linear(embed_size, embed_size)\n",
    "        self.w_k = nn.Linear(embed_size, embed_size)\n",
    "        self.w_v = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "        \n",
    "        参数:\n",
    "            q: 查询矩阵 (batch_size, seq_len_q, embed_size)\n",
    "            k: 键矩阵 (batch_size, seq_len_k, embed_size)\n",
    "            v: 值矩阵 (batch_size, seq_len_v, embed_size)\n",
    "            mask: 掩码矩阵，用于屏蔽不应关注的位置 (batch_size, seq_len_q, seq_len_k)\n",
    "\n",
    "        返回:\n",
    "            out: 注意力加权后的输出\n",
    "            attention_weights: 注意力权重矩阵\n",
    "        \"\"\"\n",
    "        Q = self.w_q(q)\n",
    "        K = self.w_k(k)\n",
    "        V = self.w_v(v)\n",
    "        out, attention_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e12e73a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        \"\"\"\n",
    "        自注意力（Self-Attention）机制。\n",
    "        \n",
    "        参数:\n",
    "            embed_size: 输入序列的嵌入维度（每个向量的特征维度）。\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.attention = Attention(embed_size)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "        \n",
    "        参数:\n",
    "            x: 输入序列 (batch_size, seq_len, embed_size)\n",
    "            mask: 掩码矩阵 (batch_size, seq_len, seq_len)\n",
    "\n",
    "        返回:\n",
    "            out: 自注意力加权后的输出 (batch_size, seq_len, embed_size)\n",
    "            attention_weights: 注意力权重矩阵 (batch_size, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        # 在自注意力机制中，q, k, v 都来自同一输入序列\n",
    "        # q = k = v = x\n",
    "        out, attention_weights = self.attention(x, x, x, mask)\n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad6b232c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        \"\"\"\n",
    "        交叉注意力（Cross-Attention）机制。\n",
    "        \n",
    "        参数:\n",
    "            embed_size: 输入序列的嵌入维度。\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.attention = Attention(embed_size)\n",
    "    \n",
    "    def forward(self, q, kv, mask=None):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "        \n",
    "        参数:\n",
    "            query: 查询矩阵的输入 (batch_size, seq_len_q, embed_size)\n",
    "            kv: 键和值矩阵的输入 (batch_size, seq_len_kv, embed_size)\n",
    "            mask: 掩码矩阵 (batch_size, seq_len_q, seq_len_kv)\n",
    "\n",
    "        返回:\n",
    "            out: 注意力加权后的输出 (batch_size, seq_len_q, embed_size)\n",
    "            attention_weights: 注意力权重矩阵 (batch_size, seq_len_kv, seq_len_kv)\n",
    "        \"\"\"\n",
    "        # 在交叉注意力机制中，q 和 k, v 不同\n",
    "        # q 来自解码器，k 和 v 来自编码器（观察模型架构图）\n",
    "        out, attention_weights = self.attention(q, kv, kv, mask)\n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb9f213e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    缩放点积注意力计算。\n",
    "    参数:\n",
    "        Q: 查询矩阵 (batch_size, num_heads, seq_len_q, head_dim)\n",
    "        K: 键矩阵 (batch_size, num_heads, seq_len_k, head_dim)\n",
    "        V: 值矩阵 (batch_size, num_heads, seq_len_v, head_dim)\n",
    "        mask: 掩码矩阵 (1, 1, seq_len_q, seq_len_k) 或 (batch_size, 1, seq_len_q, seq_len_k) 或 (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "\n",
    "    返回:\n",
    "        output: 注意力加权后的输出矩阵\n",
    "        attention_weights: 注意力权重矩阵\n",
    "    \"\"\"\n",
    "    embed_size = Q.size(-1)\n",
    "    scores = nn.matmul(Q, K.transpose(-2,-1))/math.sqrt(embed_size)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(0,float('-inf'))\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    output = nn.matmul(attention_weights, V)\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "afad6bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, h):\n",
    "        \"\"\"\n",
    "        多头注意力机制：每个头单独定义线性层。\n",
    "        \n",
    "        参数:\n",
    "            d_model: 输入序列的嵌入维度。\n",
    "            h: 注意力头的数量。\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert d_model % h == 0, \"d_model 必须能被 h 整除。\"\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "        \n",
    "        参数:\n",
    "            q: 查询矩阵 (batch_size, seq_len_q, d_model)\n",
    "            k: 键矩阵 (batch_size, seq_len_k, d_model)\n",
    "            v: 值矩阵 (batch_size, seq_len_v, d_model)\n",
    "            mask: 掩码矩阵 (batch_size, 1, seq_len_q, seq_len_k)\n",
    "\n",
    "        返回:\n",
    "            out: 注意力加权后的输出\n",
    "            attention_weights: 注意力权重矩阵\n",
    "        \"\"\"\n",
    "        batch_size = q.size(0)\n",
    "        seq_len_q = q.size(1)\n",
    "        seq_len_k = v.size(1)\n",
    "        Q = self.w_q(q).view(batch_size, seq_len_q, self.h, -1).transpose(1,2)\n",
    "        K = self.w_k(k).view(batch_size, seq_len_k, self.h, -1).transpose(1,2)\n",
    "        V = self.w_v(v).view(batch_size, seq_len_k, self.h, -1).transpose(1,2)\n",
    "        scaled_attention, _ = scaled_dot_product_attention(Q, K, V, mask)\n",
    "        concat_out = scaled_attention.transpose(1,2).contigous().view(batch_size, -1, self.d_model)\n",
    "        out = self.fc_out(concat_out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0474f278",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositonwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        位置前馈网络。\n",
    "        \n",
    "        参数:\n",
    "            d_model: 输入和输出向量的维度\n",
    "            d_ff: FFN 隐藏层的维度，或者说中间层\n",
    "            dropout: 随机失活率（Dropout），即随机屏蔽部分神经元的输出，用于防止过拟合\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(self.w_1(x).relu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a17a4b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        \"\"\"\n",
    "        残差连接，用于在每个子层后添加残差连接和 Dropout。\n",
    "        \n",
    "        参数:\n",
    "            dropout: Dropout 概率，用于在残差连接前应用于子层输出，防止过拟合。\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "        \n",
    "        参数:\n",
    "            x: 残差连接的输入张量，形状为 (batch_size, seq_len, d_model)。\n",
    "            sublayer: 子层模块的函数，多头注意力或前馈网络。\n",
    "\n",
    "        返回:\n",
    "            经过残差连接和 Dropout 处理后的张量，形状为 (batch_size, seq_len, d_model)。\n",
    "        \"\"\"\n",
    "        return x+self.dropout(sublayer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3306f9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, feature_size, epsilon=1e-9):\n",
    "        \"\"\"\n",
    "        层归一化，用于对最后一个维度进行归一化。\n",
    "        \n",
    "        参数:\n",
    "            feature_size: 输入特征的维度大小，即归一化的特征维度。\n",
    "            epsilon: 防止除零的小常数。\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.gamma = nn.parameter(torch.ones(feature_size))\n",
    "        self.beta = nn.Parameter(torch.zeros(feature_size))\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        return self.gamma * ((x - mean)/(std + self.epsilon)) + self.beta\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7d03901",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    def __init__(self, feature_size, dropout=0.1, epsilon=1e-9):\n",
    "        \"\"\"\n",
    "        子层连接，包括残差连接和层归一化，应用于 Transformer 的每个子层。\n",
    "\n",
    "        参数:\n",
    "            feature_size: 输入特征的维度大小，即归一化的特征维度。\n",
    "            dropout: 残差连接中的 Dropout 概率。\n",
    "            epsilon: 防止除零的小常数。\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.residual = ResidualConnection(dropout)\n",
    "        self.norm = LayerNorm(feature_size, epsilon)\n",
    "    \n",
    "    def forward(self, x, sublayer):\n",
    "        return self.norm(self.residual(x, sublayer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475444a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        \"\"\"\n",
    "        嵌入，将 token ID 转换为固定维度的嵌入向量，并进行缩放。\n",
    "\n",
    "        参数:\n",
    "            vocab_size: 词汇表大小。\n",
    "            d_model: 嵌入向量的维度。\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.scale_factor = math.sqrt(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "\n",
    "        参数:\n",
    "            x: 输入张量，形状为 (batch_size, seq_len)，其中每个元素是 token ID。\n",
    "\n",
    "        返回:\n",
    "            缩放后的嵌入向量，形状为 (batch_size, seq_len, d_model)。\n",
    "        \"\"\"\n",
    "        return self.embed(x) * self.scale_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eb330d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        \"\"\"\n",
    "        位置编码，为输入序列中的每个位置添加唯一的位置表示，以引入位置信息。\n",
    "\n",
    "        参数:\n",
    "            d_model: 嵌入维度，即每个位置的编码向量的维度。\n",
    "            dropout: 位置编码后应用的 Dropout 概率。\n",
    "            max_len: 位置编码的最大长度，适应不同长度的输入序列。\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:,0::2] = torch.sin(position * div_term)\n",
    "        pe[:,1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "\n",
    "        参数:\n",
    "            x: 输入序列的嵌入向量，形状为 (batch_size, seq_len, d_model)。\n",
    "\n",
    "        返回:\n",
    "            加入位置编码和 Dropout 后的嵌入向量，形状为 (batch_size, seq_len, d_model)。\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:,:x.size(1),:]  \n",
    "        return self.dropout(x)     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
